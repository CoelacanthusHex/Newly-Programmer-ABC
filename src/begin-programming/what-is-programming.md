# 什么是编程？

简单地说，编程是人们为了解决问题，**将解决问题的步骤，用计算机能够理解的语言描述出来**。而后，计算机逐步执行这些指令，最后完成任务。

编程最核心的，就是以计算机能够理解的方式设计解决问题的步骤，这也正是编程的难点所在。

## 机器指令

那么什么是计算机能理解的语言呢？我们说，计算机的核心是计算，而实现计算的，是计算机的“中央处理器”，也就是我们说的 CPU。而计算机能理解的语言，就是 CPU 能理解的语言。

我们把 CPU 的语言叫做“**机器指令**”或者“**机器码**”。由于我们现如今的计算机几乎都是二进制的，“机器码”也就是以二进制形式表示的数码。

CPU 内部是复杂的电路，根据事先约定好的方式精心设计而成，它能够理解我们输入的数码、进行计算，并输出结果。一条指令，会让 CPU 做一次运算；编程，就是很多指令的形成的序列。

假如我们要计算“$12 + 34$”，我们可以按照约定好的方式告诉 CPU 要做的事情——“计算加法，加数和被加数分别为 12 和 34”。而实际上输入 CPU 的，是一条指令。假如“00001”代表“做加法”，而“12”“34”的二进制分别为“1100”“100010”，那么 CPU 接收到的的指令可能是下面这个样子：

```text
00001 0000 0000 0000 1100 0000 0000 0010 0010
```

你可能会注意到，这次加法运算中的两个数据被补齐到了 16 位，以固定长度编码数据方便我们设计 CPU 内部的电路以及编写指令。此外，我们可以认为这台 CPU 能够处理（最长）16 位的数据，因此是一个 16 位的 CPU，也称机器的“字长”为 16 位。

同理，假如“00002”代表减法，那么计算“$12 - 34$”的指令可能是：

```text
00002 0000 0000 0000 1100 0000 0000 0010 0010
```

上面只是一个简化的举例，实际情况中，CPU 指令的设计是一个比较复杂的事情。一台 CPU 能够支持的各种指令叫做“指令集”，通常有“复杂指令集”和“简单指令集”之分。

## 编程让 CPU 实现复杂一点的运算

刚才我们尝试进行了加减法的计算，这只需要进行一次运算就可以得到结果。但是很多计算过程都需要不止一次运算来实现——比如求数的阶乘、求数列的和等。

以计算 10 的阶乘为例，我们可能需要这样编写指令：

```text
计算 1 * 2
结果（3） * 3
结果（9） * 4
结果（36） * 5
...
结果（544320） * 10
```

我们发现，这样编写指令效率很低，因为我们需要不断地得到结果，并根据结果编写新的指令。我们想，如果 CPU 能够记录下中间的结果就好了。现代 CPU 中确实有这样的设计，即“寄存器（register）”。如果我们用“`$1`”表示寄存器 1 号，则刚刚的程序可以这样改写：

```text
$1 = 1
$1 = $1 * 2
$1 = $1 * 3
$1 = $1 * 4
...
$1 = $1 * 10
```

这样子，我们虽然还是要写 10 行命令，但是可以一次性地将这个指令序列编写好，依次输入 CPU 即可。

不过我们现在又发现，这些指令之间大部分都是相似的，只有一个变数，并且还是有规律的变化（每次递增 1）。于是，我们可以考虑将这个变化的值也放入寄存器中。

然后我们可以这样写程序

```text
$1 = 1
$2 = 2

$1 = $1 * $2
$2 增加 1

重复前 2 条指令 9 次
```

这样，我们就构造了一个“循环”，可以看到，使用循环之后的，程序的代码量大大降低。

不过，CPU 通常没有智能到能够明白我们所谓的“重复之前指令”。我们一般只能让 CPU 跳转到某个位置，如“2 条指令之前”。为此，我们还需要将代码改写一下：

```text
$1 = 1
$2 = 2

若 $2 大于 10，则跳转到 3 条指令后
$1 = $1 * $2
$2 增加 1 
跳转到 3 条指令前
```

我们通过在一段指令的结束处**跳转到之前的位置**来**实现循环**，并**在循环体的入口做判断**，来决定是否跳出循环。

以上便是 CPU 所能理解的语言，即机器语言。上述讲解过程中，我们避开了二进制数码的形式，而是用人类使用的语言描述了指令所实现的操作。

## 汇编语言

要记忆二进制数码对人类来说是件不太容易的事，于是人类发明了助记符，也就是用字母或单词替换机器指令中的二进制数码来编写程序，这便是汇编语言。汇编语言经过**汇编器**变成机器码。

## 高级语言

在前文中我们看到，汇编语言的编写难度依旧很高，比如，一个循环，需要使用指令的跳转来实现。这在编程过程中容易犯错，并且不容易被发现。此外，不同 CPU 的指令集不同，这意味着，同样的计算过程，需要针对不同的 CPU 上编写其所能理解的机器指令。

于是，人类又发明了诸多**高级语言**，用来描述算法。高级语言往往有着更清晰的语法，且与平台无关，再一次降低了编程的难度。高级语言通过词法分析、编译等步骤，最终变成 CPU 能够理解的机器指令。

> 为了提高开发效率，我们常采用将多种高级语言翻译成一种**中间语言**（中间码）、再将中间码翻译成**针对不同平台的机器码**的方式。这种方式使得开发人员不必掌握从高级语言到具体平台机器码的全部知识——只需要专注高级语言翻译到中间码的过程和中间码到机器码的过程，降低了开发成本，同时也使协同开发更加方便，提升了开发效率。此外，我们也可以只专注于中间码的优化，这会更加方便，比如我们可以很容易的提取出某些特定形式的指令序列，并用等价的、更高效的指令进行替换。

当然，高级语言的编译器也是一个程序，其也是（具体平台上的）一串机器码。高级语言的编译器最开始也需要由汇编语言写就，或者由现有的高级语言编写并编译而成。

要用汇编语言实现如词法分析等如此复杂的功能，听起来好像是个很困难的事。不过事实上，我们可以通过编写一个能够分析处理一些基础的语法的简单编译器，然后再用这些基础的语法实现更高级的功能，并用这个简单编译器编译得到更高级的编译器。这种后一步在前一步基础上前进的过程叫做“自举（bootstrap）”。

常见的高级语言有 C、C++、Java 等。

大多数情况下，编译器在将我们写就的高级语言翻译成机器指令时，会尽可能地利用平台的特性以及做出必要的优化。（当然，不同编译器的优化能力也不尽相同。）

尽管如此，程序员仍需对硬件有一定的了解，这样才能写出效率更高的程序。
